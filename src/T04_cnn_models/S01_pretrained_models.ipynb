{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e166f44e",
   "metadata": {},
   "source": [
    "# Classification Model\n",
    "\n",
    "- https://docs.pytorch.org/vision/main/models.html#classification\n",
    "\n",
    "Ask AI\n",
    "> How can I initialize pretrained SqueezeNet from torchvision? I am using pytorch 2.7 and torchvision 0.22."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "921ff39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b0c19e",
   "metadata": {},
   "source": [
    "| Model           | Number of Parameters |\n",
    "|-----------------|---------------------|\n",
    "| SqueezeNet1_1   | ~1.25M              |\n",
    "| MobileNet V2    | ~3.5M               |\n",
    "| ResNet18        | ~11.7M              |\n",
    "| VGG16           | ~138M               |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2672482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Load pre-trained model (choose one by uncommenting) -----\n",
    "# model_choice = \"squeezenet1_1\"\n",
    "# model_choice = \"mobilenet_v2\"\n",
    "model_choice = \"resnet18\"\n",
    "# model_choice = \"vgg16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc4e34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the chosen pre-trained model from torchvision.models\n",
    "if model_choice == \"squeezenet1_1\":\n",
    "    model = models.squeezenet1_1(weights=models.SqueezeNet1_1_Weights.IMAGENET1K_V1)\n",
    "elif model_choice == \"mobilenet_v2\":\n",
    "    model = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1)\n",
    "elif model_choice == \"resnet18\":\n",
    "    model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "elif model_choice == \"vgg16\":\n",
    "    model = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n",
    "else:\n",
    "    raise Exception(\"Invalid model\")  # Raise an error if input is invalid\n",
    "\n",
    "# ----- Download and load ImageNet class labels from the online file -----\n",
    "label_url = \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\"\n",
    "\n",
    "labels = requests.get(\n",
    "    label_url\n",
    ").text.splitlines()  # Split text by newlines, creating a list of label strings\n",
    "\n",
    "# Print the selected model's name and the list of labels\n",
    "print(model_choice)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d834140e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests  # Import the requests library for handling HTTP requests\n",
    "\n",
    "# URL where the ImageNet class labels are stored as a plain text file\n",
    "label_url = \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\"\n",
    "\n",
    "# Send a GET request to the URL and get the content as a string\n",
    "labels = requests.get(\n",
    "    label_url\n",
    ").text.splitlines()  # Split by lines to create a list of labels\n",
    "\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232094b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "input_size = (1, 3, 224, 224)  # (batch_size, channels, height, width)\n",
    "summary(model, input_size=input_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7d2b56",
   "metadata": {},
   "source": [
    "### What is `.named_parameters()` in PyTorch?\n",
    "- `.named_parameters()` is a method provided by PyTorchâ€™s nn.Module class (the base class for all models and layers in PyTorch).\n",
    "- It allows you to iterate over all the parameters of your model, but with an added benefit: it gives you both the name of the parameter (as a string) and the parameter itself (as a torch.nn.Parameter object)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a475ac22",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"Parameter name: {name}\")\n",
    "    print(f\"Shape: {param.shape}\")\n",
    "    print(f\"Requires grad: {param.requires_grad}\")  # True if parameter will be updated\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce8b34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a sequence of preprocessing transformations for input images,\n",
    "# typically used for models trained on ImageNet.\n",
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(256),  # Resize the shortest side of the image to 256 pixels\n",
    "        transforms.CenterCrop(224),  # Crop the center 224x224 region\n",
    "        transforms.ToTensor(),  # Convert the PIL image to a PyTorch tensor and scale to [0, 1]\n",
    "        transforms.Normalize(\n",
    "            mean=[\n",
    "                0.485,\n",
    "                0.456,\n",
    "                0.406,\n",
    "            ],  # Normalize each channel using ImageNet dataset mean\n",
    "            std=[\n",
    "                0.229,\n",
    "                0.224,\n",
    "                0.225,\n",
    "            ],  # Normalize each channel using ImageNet dataset std deviation\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c66958",
   "metadata": {},
   "source": [
    "### Upload image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f4ceaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "image_data = dict(image=None)\n",
    "\n",
    "\n",
    "def on_upload_change(change):\n",
    "    file_content = uploader.value[0][\"content\"]\n",
    "    image = Image.open(io.BytesIO(file_content)).convert(\"RGB\")\n",
    "    image_data[\"image\"] = image\n",
    "    display(image)  # Show the image\n",
    "\n",
    "\n",
    "uploader = widgets.FileUpload(accept=\"image/*\", multiple=False)\n",
    "display(uploader)\n",
    "uploader.observe(on_upload_change, names=\"value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5106e0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if image_data[\"image\"] is not None:\n",
    "    input_tensor = preprocess(image_data[\"image\"])\n",
    "    input_batch = input_tensor.unsqueeze(0)\n",
    "    print(input_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ad70b2",
   "metadata": {},
   "source": [
    "### Check what actually goes into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4c5dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if image_data[\"image\"] is not None:\n",
    "    # Get the image tensor's dimensions: (channels, height, width)\n",
    "    _, height, width = input_tensor.shape\n",
    "\n",
    "    # Reorder tensor axes to (height, width, channels) for displaying with matplotlib\n",
    "    img = input_tensor.permute(1, 2, 0).numpy()\n",
    "\n",
    "    # -- Min-max normalization to scale pixel values to the [0, 1] range --\n",
    "    min_val = img.min()\n",
    "    max_val = img.max()\n",
    "    img = (img - min_val) / (max_val - min_val)\n",
    "\n",
    "    # Display the image without axes\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # Overlay the image resolution on the image (top-left corner) with a background box for readability\n",
    "    plt.text(\n",
    "        5,\n",
    "        15,\n",
    "        f\"{width} x {height}\",\n",
    "        color=\"white\",\n",
    "        bbox=dict(facecolor=\"black\", alpha=0.5),\n",
    "        fontsize=12,\n",
    "    )\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30604be",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b0d781",
   "metadata": {},
   "outputs": [],
   "source": [
    "if image_data[\"image\"] is not None:\n",
    "    # Set the model to evaluation mode (important for layers like dropout or batchnorm)\n",
    "    model.eval()\n",
    "\n",
    "    # Temporarily disable gradient calculation to speed up computation and save memory\n",
    "    with torch.no_grad():\n",
    "        # Forward pass: get the model's raw output (logits) for the input batch\n",
    "        output = model(input_batch)\n",
    "\n",
    "    # Apply softmax to convert logits to probabilities for the first item in the batch\n",
    "    probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "\n",
    "    # Get the top 5 probabilities and their category indices\n",
    "    top5_prob, top5_catid = torch.topk(probabilities, 5)\n",
    "\n",
    "    print(\"\\nðŸŒŸ Top-5 Predictions ðŸŒŸ\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"{'Rank':<5} {'Label':<25} {'Confidence':>10}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    for i in range(top5_prob.size(0)):\n",
    "        rank = i + 1\n",
    "        label = labels[top5_catid[i]]\n",
    "        percent = top5_prob[i].item() * 100\n",
    "        print(f\"{rank:<5} {label:<25} {percent:>8.2f}%\")\n",
    "\n",
    "    print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
